{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2QKFIlR6HCY"
   },
   "source": [
    "# PageRank\n",
    "\n",
    "In this assignment, you will compute PageRank on a collection of 469,235 web sites using the iterative version of the PageRank algorithm described in class for sparse graphs (NOT the power method with explicit matrix multiplication).\n",
    "\n",
    "Consider the following directed graph:\n",
    "\n",
    "![A directed link graph](https://ccs.neu.edu/home/dasmith/courses/cs6200/pagerank.jpg)\n",
    "\n",
    "We can represent this graph as a collection of nodes, here, ordered pairs of node index and node name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4D_Mxj5pXKPl"
   },
   "outputs": [],
   "source": [
    "small_nodes = [(0, 'A'),\n",
    "              (1, 'B'),\n",
    "              (2, 'C'),\n",
    "              (3, 'D'),\n",
    "              (4, 'E'),\n",
    "              (5, 'F')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RTF3JKtTYxiZ"
   },
   "source": [
    "and a collection of directed links, i.e., ordered pairs from source to target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-i0V5ueOYDDN"
   },
   "outputs": [],
   "source": [
    "small_edges = [\n",
    "  (0, 1),\n",
    "  (0, 2),\n",
    "  (0, 5),\n",
    "  (1, 2),\n",
    "  (1, 3),\n",
    "  (1, 4),\n",
    "  (1, 5),\n",
    "  (2, 3),\n",
    "  (2, 4),\n",
    "  (3, 0),\n",
    "  (3, 2),\n",
    "  (3, 4),\n",
    "  (3, 5),\n",
    "  (4, 0),\n",
    "  (5, 0),\n",
    "  (5, 1),\n",
    "  (5, 4)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kBVDeszXY4B_"
   },
   "source": [
    "We use integer identifiers for the nodes for efficiency. Note that, unlike this example, in a real web graph, not every page will have in-links, nor will every page have out-links."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy\n",
    "Entropy is a measure of the uncertainty in a dataset. In the context of data compression, it quantifies the limit to which a dataset can be compressed. Mathematically, the entropy \\( H(X) \\) is defined as:\n",
    "\n",
    "$$H(X) = -\\sum_{i=1}^{n} p(x_i) \\log_2 p(x_i)$$\n",
    "\n",
    "### Perplexity\n",
    "Perplexity is a metric that quantifies how well a probability distribution predicts a sample, and it is directly related to entropy. In this PageRank assignment, we define perplexity as $2^{H(PR)}$, where $H(PR)$ is the entropy of the PageRank distribution.\n",
    "\n",
    "### Why It's Important\n",
    "Calculating the perplexity at each iteration helps us understand to what extent the PageRank distribution is nearing a steady state. In other words, if the perplexity is changing very little across iterations, then we can assume that the PageRank distribution has stabilized, and the iterations can be stopped.\n",
    "\n",
    "### Threshold Selection\n",
    "Regarding the threshold, a common practice is to set a very small positive number (e.g., $1 \times 10^{-5}$). If the change in perplexity across consecutive iterations is smaller than this threshold, then the iterations can be stopped.\n",
    "\n",
    "### Maximum Perplexity\n",
    "The maximum perplexity of a PageRank distribution will be equal to the number of nodes in the graph. This is because when all nodes have equal PageRank values, the entropy will reach its maximum, thus maximizing the perplexity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QPNsTGSsEwMX"
   },
   "source": [
    "\n",
    "## First Implementation and Test\n",
    "\n",
    "\\[10 points\\] Implement the iterative PageRank algorithm. Test your code on the six-node example using the input representation given above.  Be sure that your code handles pages that have no in-links or out-links properly.  (You may wish to test on a few such examples.) In later parts of this assignment, depending on how you store the data, it may be convenient to use iterators rather than storing the data in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NMu_WaDA55sk"
   },
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Implement PageRank, given nodes and edges, to start with a uniform\n",
    "# distribution over nodes, run a fixed number of iterations, and\n",
    "# return a distribution over nodes.\n",
    "\n",
    "def page_rank_fixed_iter(nodes, edges, iterations=10):\n",
    "    \n",
    "    P = set()\n",
    "    S = set()\n",
    "    M = dict()\n",
    "    L = dict()\n",
    "    N = len(nodes)\n",
    "    d = 0.85\n",
    "\n",
    "    page_rank = dict()\n",
    "\n",
    "\n",
    "    for node in nodes:\n",
    "        P.add(node[0])\n",
    "\n",
    "    S = P.copy()\n",
    "\n",
    "    for edge in edges:\n",
    "\n",
    "        if edge[0] in S:\n",
    "            \n",
    "            S.remove(edge[0])\n",
    "        \n",
    "        if edge[1] in M:\n",
    "\n",
    "            M[edge[1]].append(edge[0])\n",
    "        \n",
    "        else:\n",
    "\n",
    "            M[edge[1]] = [edge[0]]\n",
    "        \n",
    "\n",
    "        if edge[0] in L:\n",
    "\n",
    "            L[edge[0]].append(edge[1])\n",
    "        \n",
    "        else:\n",
    "\n",
    "            L[edge[0]] = [edge[1]]\n",
    "\n",
    "    for page in P:\n",
    "\n",
    "        page_rank[page] = 1 / len(P)\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    while i < iterations:\n",
    "\n",
    "        sinkPR = 0\n",
    "\n",
    "        for page in S:\n",
    "\n",
    "            sinkPR += page_rank[page]\n",
    "        \n",
    "        for page in P:\n",
    "\n",
    "            new_page_rank = (1- d) / N\n",
    "            new_page_rank += d * (sinkPR/N)\n",
    "\n",
    "            for neighbor in M[page]:\n",
    "\n",
    "                new_page_rank += d * page_rank[neighbor] / len(L[neighbor])\n",
    "            \n",
    "            page_rank[page] = new_page_rank\n",
    "        \n",
    "        i += 1\n",
    "    \n",
    "    \n",
    "    return page_rank\n",
    "\n",
    "print(page_rank_fixed_iter(small_nodes, small_edges, 1))\n",
    "print()\n",
    "\n",
    "print(page_rank_fixed_iter(small_nodes, small_edges, 10))\n",
    "print()\n",
    "\n",
    "print(page_rank_fixed_iter(small_nodes, small_edges, 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4duRjzABB9n"
   },
   "source": [
    "## PageRank on Web Crawl Data\n",
    "\n",
    "\\[20 points\\] Download and unpack a list of `.edu` websites and the links among them from the [Common Crawl](https://commoncrawl.org/2017/05/hostgraph-2017-feb-mar-apr-crawls/) open-source web crawl. For the sake of brevity, the data record links among websites, not web pages. The information for nodes and links is the same as the toy example above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v6EDDdTQCd3y"
   },
   "outputs": [],
   "source": [
    "# If you're running on a machine (e.g., Windows) that doesn't have wget or gzip,\n",
    "# feel free to comment this out and use a different set of commands to load\n",
    "# the data.\n",
    "\n",
    "!wsl wget https://ccs.neu.edu/home/dasmith/courses/cs6200/vertices-edu.txt.gz\n",
    "!wsl gzip -df vertices-edu.txt.gz\n",
    "!wsl wget https://ccs.neu.edu/home/dasmith/courses/cs6200/edges-edu.txt.gz\n",
    "!wsl gzip -df edges-edu.txt.gz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CW4yp1gPUwzb"
   },
   "source": [
    "There should now be files `vertices-edu.txt` and `edges-edu.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ly1t9fyjK7eC"
   },
   "outputs": [],
   "source": [
    "# TODO: Process the raw data into the same format as the simple graph.\n",
    "# You may create lists or iterators.\n",
    "\n",
    "nodes = []\n",
    "edges = []\n",
    "\n",
    "with open(\"vertices-edu.txt\", \"r\") as file:\n",
    "\n",
    "    for line in file:\n",
    "\n",
    "        content = line.strip().split()\n",
    "        content[0] = int(content[0])\n",
    "        nodes.append(tuple(content))\n",
    "\n",
    "with open(\"edges-edu.txt\", \"r\") as file:\n",
    "\n",
    "    for line in file:\n",
    "\n",
    "        content = line.strip().split()\n",
    "        content[0] = int(content[0])\n",
    "        content[1] = int(content[1])\n",
    "        edges.append(tuple(content))\n",
    "\n",
    "print(nodes[:10])\n",
    "print(edges[:10])\n",
    "\n",
    "print(\"Nodes length is \" + str(len(nodes)))\n",
    "print(\"Edges length is \" + str(len(edges)))\n",
    "\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0WMf5L5VEqZb"
   },
   "source": [
    "Refine your implementation of PageRank to test for numerical convergence. Specificially, at each iteration, calculate the [perplexity](https://en.wikipedia.org/wiki/Perplexity) of the PageRank distribution, where perplexity is defined as 2 raised to the [Shannon entropy](https://en.wikipedia.org/wiki/Entropy_(information_theory)) of the PageRank distribution, i.e., $2^{H(PR)}$. (Recall that we defined entropy when talking about data compression.) The maximum perplexity of a PageRank distribution will therefore be the number of nodes in the graph.\n",
    "\n",
    "At each iteration, check the _change_ in perplexity. If the change is less than some threshold, you can stop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wsL0yQKvKqAC"
   },
   "outputs": [],
   "source": [
    "# TODO: Implement convergence testing in PageRank\n",
    "# If you choose, you can share some subroutines with your first version.\n",
    "# Print the change in perplexity at each iteration.\n",
    "\n",
    "import math\n",
    "\n",
    "def page_rank(nodes, edges, threshold=1):\n",
    "    \n",
    "    P = set()\n",
    "    S = set()\n",
    "    M = dict()\n",
    "    L = dict()\n",
    "    N = len(nodes)\n",
    "    d = 0.85\n",
    "\n",
    "    print(N)\n",
    "\n",
    "    page_rank = dict()\n",
    "\n",
    "\n",
    "    for node in nodes:\n",
    "        \n",
    "        P.add(node[0])\n",
    "\n",
    "    S = P.copy()\n",
    "\n",
    "    for edge in edges:\n",
    "\n",
    "        if edge[0] in S:\n",
    "            \n",
    "            S.remove(edge[0])\n",
    "        \n",
    "        if edge[1] in M:\n",
    "\n",
    "            M[edge[1]].add(edge[0])\n",
    "        \n",
    "        else:\n",
    "\n",
    "            M[edge[1]] = {edge[0]}\n",
    "        \n",
    "\n",
    "        if edge[0] in L:\n",
    "\n",
    "            L[edge[0]].add(edge[1])\n",
    "        \n",
    "        else:\n",
    "\n",
    "            L[edge[0]] = {edge[1]}\n",
    "    \n",
    "\n",
    "    for page in P:\n",
    "\n",
    "        page_rank[page] = 1 / len(P)\n",
    "\n",
    "    perplexity = N\n",
    "    change_in_perplexity = float('inf')\n",
    "\n",
    "    temp_page_ranks = dict()\n",
    "\n",
    "    while change_in_perplexity > threshold:\n",
    "\n",
    "        shannon_entropy = 0\n",
    "\n",
    "        sinkPR = 0\n",
    "\n",
    "        for page in S:\n",
    "            \n",
    "            sinkPR += page_rank[page]\n",
    "        \n",
    "        for page in P:\n",
    "\n",
    "            new_page_rank = (1- d) / N\n",
    "            new_page_rank += d * (sinkPR/N)\n",
    "\n",
    "            if page in M:\n",
    "\n",
    "                for neighbor in M[page]:\n",
    "\n",
    "                    new_page_rank += d * page_rank[neighbor] / len(L[neighbor])\n",
    "                \n",
    "                temp_page_ranks[page] = new_page_rank\n",
    "\n",
    "                shannon_entropy += temp_page_ranks[page] * math.log(1/temp_page_ranks[page], 2) \n",
    "        \n",
    "        for page in temp_page_ranks.keys():\n",
    "            \n",
    "            page_rank[page] = temp_page_ranks[page]\n",
    "        \n",
    "        shannon_entropy *= -1\n",
    "        \n",
    "        previous_perplexity = perplexity\n",
    "        print(\"Previous Perplexity is \" + str(previous_perplexity))\n",
    "\n",
    "        updated_perplexity = math.pow(2, shannon_entropy)\n",
    "        print(\"Updated Perplexity is \" + str(updated_perplexity))\n",
    "\n",
    "        change_in_perplexity = abs(previous_perplexity - updated_perplexity)\n",
    "\n",
    "        perplexity = updated_perplexity\n",
    "\n",
    "        print(\"Change in perplexity is \" + str(change_in_perplexity))\n",
    "    \n",
    "    return page_rank\n",
    "\n",
    "PR = page_rank(nodes, edges, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rcncY2QHNl0M"
   },
   "source": [
    "## Link Analysis\n",
    "\n",
    "\\[20 points\\] In this final section, you will compute some properties of the web-site graph you downloaded.\n",
    "\n",
    "First, consider the _in-link count_ of a website, simply the number of web-sites pointing to it (including self-links). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a_FyPlLSO2bu"
   },
   "outputs": [],
   "source": [
    "# TODO: List the document ID, domain name, and in-link count of the 60 websites with the highest in-link count\n",
    "\n",
    "M = dict()\n",
    "\n",
    "for edge in edges:\n",
    "\n",
    "   if edge[1] in M:\n",
    "\n",
    "      M[edge[1]].add(edge[0])\n",
    "   \n",
    "   else:\n",
    "\n",
    "      M[edge[1]] = {edge[0]}\n",
    "\n",
    "sorted_dict = dict(sorted(M.items(), key=lambda item : len(item[1]), reverse=True))\n",
    "\n",
    "count = 0\n",
    "\n",
    "for key, value in sorted_dict.items():\n",
    "   \n",
    "   if count >= 60:\n",
    "      break\n",
    "   \n",
    "   print(f\"Document ID : {key}, Domain Name : {len(sorted_dict[key])}, In-Link Count : {len(value)}\")\n",
    "\n",
    "   count += 1\n",
    "\n",
    "print(\"Count is \" + str(count))\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5uSlQEtmPTTA"
   },
   "source": [
    "Then, use the PageRank values compute by your second implementation. Note that some websites will have both a high in-link count and PageRank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nwcci2kdPlMR"
   },
   "outputs": [],
   "source": [
    "# TODO: List the document ID, domain name, and PageRank of the 60 websites with the highest PageRank.\n",
    "\n",
    "sorted_page_ranks = dict(sorted(PR.items(), key=lambda item : item[1], reverse=True))\n",
    "\n",
    "count = 0\n",
    "\n",
    "for key, value in sorted_page_ranks.items():\n",
    "   \n",
    "   if count >= 60:\n",
    "      break\n",
    "   \n",
    "   print(f\"Document ID : {key}, Domain Name : {nodes[key]}, Page Rank : {value}\")\n",
    "\n",
    "   count += 1\n",
    "\n",
    "print(\"Count is \" + str(count))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SxODBxL_Pyy2"
   },
   "source": [
    "Finally, compute some summary statistics on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oD4bq6AyQIsU"
   },
   "outputs": [],
   "source": [
    "# TODO: Compute:\n",
    "# - the proportion of websites with no in-links (i.e., source nodes);\n",
    "\n",
    "source_node_count = 0\n",
    "\n",
    "for node in nodes:\n",
    "\n",
    "    if node[0] not in M:\n",
    "\n",
    "        source_node_count += 1\n",
    "\n",
    "result = source_node_count / len(nodes)\n",
    "\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
